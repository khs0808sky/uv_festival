{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23665e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ğŸš€ 'analysis_tools.py' íŒŒì¼ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---\n",
      "  [analysis_tools] 1. PDF ë¶„ì„ ì‹œì‘: ./samples/ì œ7íšŒ ë‹´ì–‘ì‚°íƒ€ì¶•ì œ ê³¼ì—…ì§€ì‹œ ë° ì œì•ˆìš”ì²­ì„œ(ìµœì¢…)-20250910.pdf\n",
      "    - PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. (ì´ 28902ì)\n",
      "    - AI ìš”ì•½ ì™„ë£Œ.\n",
      "\n",
      "[PDF ë¶„ì„ ê²°ê³¼ (JSON)]\n",
      "{\n",
      "  \"ì¶•ì œëª…\": \"ì œíšŒ ë‹´ì–‘ ì‚°íƒ€ ì¶•ì œ\",\n",
      "  \"í–‰ì‚¬ê¸°ê°„\": \"ê³„ì•½ì¼ë¡œë¶€í„° í–‰ì‚¬ì¢…ë£Œ í›„ ì¼ê¹Œì§€\",\n",
      "  \"ì˜ˆì‚°\": \"ê¸ˆì•¡ì› (ë¶€ê°€ê°€ì¹˜ì„¸ í¬í•¨)\",\n",
      "  \"í–‰ì‚¬ì¥ì†Œ\": \"ë©”íƒ€ëœë“œ ì¼ì›\",\n",
      "  \"ì£¼ìµœì£¼ê´€\": \"ë‹´ì–‘êµ°, ë‹´ì–‘ì‚°íƒ€ì¶•ì œì¶”ì§„ìœ„ì›íšŒ\",\n",
      "  \"í–‰ì‚¬ë‚´ìš©\": {\n",
      "    \"ê¸°ê°„\": \"7ì¼ê°„\",\n",
      "    \"ì£¼ìš”ë‚´ìš©\": \"ì‚°íƒ€ì¶•ì œ, ê³µì—° ë° ì²´í—˜, ì•¼ê°„ê²½ê´€ ë° í¬í† ì¡´ ì¡°ì„± ë“±\"\n",
      "  },\n",
      "  \"ì¶•ì œì¶”ì§„ë°©í–¥\": {\n",
      "    \"ì£¼ìš”ë‚´ìš©\": [\n",
      "      \"ê´€ëŒê°ì´ ì£¼ì¸ê³µì¸ ì°¸ì—¬í˜• ê´€ëŒê° ì£¼ë„í˜• ì¶•ì œ\",\n",
      "      \"ìƒê¶Œê³¼ ì˜ˆìˆ ì˜ ë§Œë‚¨\",\n",
      "      \"ì²´ë¥˜í˜•, ì•¼ê°„ ì²´í—˜ ê°€ëŠ¥í•œ ì¶•ì œ\",\n",
      "      \"ë‚¨ë…€ë…¸ì†Œ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì¶•ì œ\",\n",
      "      \"ì•ˆì „í•˜ê³  ì¹œí™˜ê²½ì ì¸ ì¶•ì œ\"\n",
      "    ]\n",
      "  },\n",
      "  \"ì…ì°°ì •ë³´\": {\n",
      "    \"ì…ì°°ë°©ë²•\": \"ì œí•œê²½ìŸì…ì°°\",\n",
      "    \"ê³„ì•½ë°©ë²•\": \"í˜‘ìƒì— ì˜í•œ ê³„ì•½\",\n",
      "    \"ì…ì°°ê³µê³ ê¸°ê°„\": \"2025. 9. 11. ~ 9. 30.\",\n",
      "    \"ì œì•ˆì„œ ì ‘ìˆ˜ì¼ì‹œ\": \"2025. 9. 30. (í™”) 13:00~17:00\",\n",
      "    \"í‰ê°€ê¸°ì¤€\": {\n",
      "      \"ê¸°ìˆ ëŠ¥ë ¥í‰ê°€\": \"80ì \",\n",
      "      \"ì •ëŸ‰í‰ê°€\": \"20ì \",\n",
      "      \"ì •ì„±í‰ê°€\": \"60ì \",\n",
      "      \"ê°€ê²©í‰ê°€\": \"20ì \"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "  [analysis_tools] 2. Google íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: ['ë‹´ì–‘ ì‚°íƒ€ ì¶•ì œ', 'í¬ë¦¬ìŠ¤ë§ˆìŠ¤']\n",
      "    âŒ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: The request failed: Google returned a response with code 429\n",
      "\n",
      "[íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬)]\n",
      "{\n",
      "  \"error\": \"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: The request failed: Google returned a response with code 429\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# analysis_tools.py\n",
    "# (PDF ë¶„ì„ + íŠ¸ë Œë“œ ë¶„ì„ì„ ë‹´ë‹¹í•˜ëŠ” 'ë„êµ¬' ëª¨ìŒ)\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- OpenAI API í‚¤ ì„¤ì • (ì´ ëª¨ë“ˆë„ AIë¥¼ ì“°ë‹ˆê¹Œ) ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"âŒ [analysis_tools] OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    openai.api_key = api_key\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ê¸°ëŠ¥ 1: PDF ë¶„ì„ê¸° (ai_summary_test.pyì—ì„œ ê°€ì ¸ì˜´)\n",
    "# ----------------------------------------------------\n",
    "def analyze_pdf(pdf_file_path):\n",
    "    \"\"\"\n",
    "    PDF íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ì„œ, AIë¡œ ìš”ì•½í•œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"  [analysis_tools] 1. PDF ë¶„ì„ ì‹œì‘: {pdf_file_path}\")\n",
    "    \n",
    "    full_text = \"\"\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        print(f\"    âŒ ì˜¤ë¥˜: '{pdf_file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return {\"error\": \"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"}\n",
    "\n",
    "    try:\n",
    "        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        doc = fitz.open(pdf_file_path)\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            full_text += page.get_text(\"text\")\n",
    "        doc.close()\n",
    "        print(f\"    - PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. (ì´ {len(full_text)}ì)\")\n",
    "\n",
    "        # 2. AIì—ê²Œ ìš”ì•½ ìš”ì²­ (v3 í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)\n",
    "        system_prompt = \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì¶•ì œ ê¸°íšì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "        (ì´í•˜ ìƒëµ ... ì´ì „ v3 í”„ë¡¬í”„íŠ¸ ë‚´ìš© ... )\n",
    "        ë§Œì•½ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, í•´ë‹¹ ê°’ì€ \"ì •ë³´ ì—†ìŒ\"ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”.\n",
    "        \"\"\"\n",
    "        # (â€» v3 í”„ë¡¬í”„íŠ¸ ì „ì²´ ë‚´ìš©ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ì–´ ì£¼ì„¸ìš”!)\n",
    "        \n",
    "        user_prompt = f\"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\\n\\n{full_text[:15000]}\"\n",
    "\n",
    "        client = openai.OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        ai_response_json_string = response.choices[0].message.content\n",
    "        print(\"    - AI ìš”ì•½ ì™„ë£Œ.\")\n",
    "        \n",
    "        # JSON ë¬¸ìì—´ì„ Python ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜\n",
    "        return json.loads(ai_response_json_string) \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return {\"error\": f\"PDF ë¶„ì„ ì˜¤ë¥˜: {e}\"}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ê¸°ëŠ¥ 2: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)\n",
    "# ----------------------------------------------------\n",
    "def get_google_trends(keywords_list):\n",
    "    \"\"\"\n",
    "    í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ, Google íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"  [analysis_tools] 2. Google íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: {keywords_list}\")\n",
    "    \n",
    "    try:\n",
    "        pytrends = TrendReq(hl='ko-KR', tz=540)\n",
    "        pytrends.build_payload(keywords_list, cat=0, timeframe='today 12-m', geo='KR')\n",
    "        \n",
    "        # (1) ì‹œê°„ë³„ ê´€ì‹¬ë„\n",
    "        interest_df = pytrends.interest_over_time()\n",
    "        \n",
    "        # (2) ì—°ê´€ ê²€ìƒ‰ì–´\n",
    "        related_queries_dict = pytrends.related_queries()\n",
    "        \n",
    "        print(\"    - íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ.\")\n",
    "        \n",
    "        # (â€» DataFrameì€ JSONìœ¼ë¡œ ë°”ë¡œ ë³´ë‚´ê¸° ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ,\n",
    "        #    ë‚˜ì¤‘ì— í•„ìš”í•œ 'ì—°ê´€ ê²€ìƒ‰ì–´'ë§Œ ë¨¼ì € ê°€ê³µí•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.)\n",
    "        \n",
    "        top_related = {}\n",
    "        for kw in keywords_list:\n",
    "            top_queries = related_queries_dict.get(kw, {}).get('top')\n",
    "            if top_queries is not None and not top_queries.empty:\n",
    "                # 'query' ì»¬ëŸ¼ì˜ ìƒìœ„ 5ê°œë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "                top_related[kw] = top_queries['query'].head(5).tolist()\n",
    "            else:\n",
    "                top_related[kw] = []\n",
    "\n",
    "        return {\n",
    "            \"analyzed_keywords\": keywords_list,\n",
    "            \"top_related_queries\": top_related\n",
    "            # \"interest_data\": interest_df.to_dict() # (í•„ìš”í•˜ë‹¤ë©´ ë‚˜ì¤‘ì— ì¶”ê°€)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # (429 ì˜¤ë¥˜ ë“±ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ)\n",
    "        print(f\"    âŒ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return {\"error\": f\"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}\"}\n",
    "    \n",
    "# ----------------------------------------------------\n",
    "# ê¸°ëŠ¥ 3: íŠ¸ë Œë“œ ë¶„ì„ê¸° (trend_test.pyì—ì„œ ê°€ì ¸ì˜´)\n",
    "# ----------------------------------------------------\n",
    "# analysis_tools.py íŒŒì¼ì— ì´ì–´ì„œ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜\n",
    "\n",
    "def get_naver_buzzwords(keyword):\n",
    "    \"\"\"\n",
    "    ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜) íƒ­ì„ í¬ë¡¤ë§í•˜ì—¬\n",
    "    'í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ' (ì—°ê´€ íƒœê·¸) ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"  [analysis_tools] 3. Naver VIEW íƒ­ ì—°ê´€ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: {keyword}\")\n",
    "    \n",
    "    # 1. ë„¤ì´ë²„ VIEW íƒ­ ê²€ìƒ‰ URL\n",
    "    # (where=viewëŠ” ë¸”ë¡œê·¸/ì¹´í˜ íƒ­ì„ ì˜ë¯¸)\n",
    "    url = f\"https://search.naver.com/search.naver?where=view&sm=tab_jum&query={keyword}\"\n",
    "    \n",
    "    # 2. (â­ï¸ì¤‘ìš”) í¬ë¡¤ë§ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ 'User-Agent' í—¤ë” ì„¤ì •\n",
    "    # (ìš°ë¦¬ê°€ 'ë¸Œë¼ìš°ì €'ì¸ ì²™ ì ‘ì†í•©ë‹ˆë‹¤)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 3. 'requests'ë¡œ HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() # 200(ì„±ê³µ) ì½”ë“œê°€ ì•„ë‹ˆë©´ ì˜¤ë¥˜ ë°œìƒ\n",
    "        \n",
    "        # 4. 'BeautifulSoup'ë¡œ HTML íŒŒì‹±(ë¶„ì„) ì¤€ë¹„\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 5. (â­ï¸ê°€ì¥ ì¤‘ìš”/ì·¨ì•½) \"í•¨ê»˜ ì°¾ëŠ” í‚¤ì›Œë“œ\"ê°€ ìˆëŠ” ì˜ì—­ ì°¾ê¸°\n",
    "        #    ë„¤ì´ë²„ëŠ” ì´ CSS ì„ íƒì(selector)ë¥¼ ìì£¼ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "        #    '.keyword_box_wrap .keyword' ë˜ëŠ” '.total_tag_area .link_tag' ë“±ì„ ì‹œë„í•©ë‹ˆë‹¤.\n",
    "        related_tags_elements = soup.select('.keyword_box_wrap .keyword')\n",
    "        \n",
    "        if not related_tags_elements:\n",
    "            # ë§Œì•½ ìœ„ ì„ íƒìê°€ ì‘ë™ ì•ˆ í•˜ë©´, 'ì—°ê´€ íƒœê·¸' ì˜ì—­ì„ ì‹œë„\n",
    "            related_tags_elements = soup.select('.total_tag_area .link_tag')\n",
    "\n",
    "        buzzwords = []\n",
    "        for tag_element in related_tags_elements:\n",
    "            # íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "            buzzword = tag_element.get_text(strip=True)\n",
    "            # '#ê´‘ì£¼ë§›ì§‘' ê°™ì€ # ê¸°í˜¸ ì œê±° (ì„ íƒ ì‚¬í•­)\n",
    "            buzzwords.append(buzzword.replace('#', ''))\n",
    "            \n",
    "        if not buzzwords:\n",
    "            print(\"    - (ì°¸ê³ ) ì—°ê´€ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë„¤ì´ë²„ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŒ)\")\n",
    "            return []\n",
    "\n",
    "        print(f\"    - Naver ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì™„ë£Œ: {buzzwords[:5]}...\") # (ë¡œê·¸ì—ëŠ” 5ê°œë§Œ)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 10ê°œë§Œ ë°˜í™˜\n",
    "        return list(dict.fromkeys(buzzwords))[:10]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Naver í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return []\n",
    "# --- (ì´ íŒŒì¼ ìì²´ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ì½”ë“œ) ---\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    \n",
    "    print(\"--- ğŸš€ 'analysis_tools.py' íŒŒì¼ ë‹¨ë… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---\")\n",
    "    \n",
    "    # 1. PDF ë¶„ì„ í…ŒìŠ¤íŠ¸\n",
    "    pdf_result = analyze_pdf(\"./samples/ì œ7íšŒ ë‹´ì–‘ì‚°íƒ€ì¶•ì œ ê³¼ì—…ì§€ì‹œ ë° ì œì•ˆìš”ì²­ì„œ(ìµœì¢…)-20250910.pdf\")\n",
    "    print(\"\\n[PDF ë¶„ì„ ê²°ê³¼ (JSON)]\")\n",
    "    print(json.dumps(pdf_result, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # 2. íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸\n",
    "    trend_result = get_google_trends([\"ë‹´ì–‘ ì‚°íƒ€ ì¶•ì œ\", \"í¬ë¦¬ìŠ¤ë§ˆìŠ¤\",])\n",
    "    print(\"\\n[íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬)]\")\n",
    "    print(json.dumps(trend_result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_festival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
