{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3424944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ ì¶•ì œëª…: ë‹´ì–‘ì‚°íƒ€ì¶•ì œ\n",
      "íƒì§€ëœ ìœ í˜•: ëŒ€='-' | ì¤‘='-' | ì„¸ë¶€='ë¬¸í™”ì˜ˆìˆ -í¬ë¦¬ìŠ¤ë§ˆìŠ¤Â·ì‚°íƒ€'  | ì¶œì²˜: dataset\n",
      "ê·¼ê±°(dataset): íŒŒì¼=2025.csv, ì—°ë²ˆ=793, ë§¤ì¹­ëª…=ì œ7íšŒ ë‹´ì–‘ì‚°íƒ€ì¶•ì œ\n",
      "\n",
      "ë™ì¼ ìœ í˜• ì¶•ì œ ìˆ˜: 2\n",
      "\n",
      "[ë™ì¼ ìœ í˜• ì¶•ì œ ëª©ë¡]\n",
      "  1. ì œ13íšŒ ìœ ëŸ¬í”¼ì•ˆ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ë§ˆì¼“\n",
      "  2. 2024 ìœ ì„±ì˜¨ì²œ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ì¶•ì œ\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ğŸ¯ (2024+2025) ì„¸ë¶„í™”ëœ ë¶„ë¥˜(ëŒ€/ì¤‘/ì„¸ë¶€) ê¸°ë°˜ ì¶•ì œ ë¶„ë¥˜ Â· ê·¼ê±° Â· ë™ì¼ì„¸ë¶€ìœ í˜• ë¦¬ìŠ¤íŠ¸\n",
    "- ë¡œë“œ: ./csv/{2024.csv, 2025.csv} (+ /mnt/data ê²½ë¡œë„ ì‹œë„)\n",
    "- ì•„ì§ ì‹œì‘ ì•ˆ í•œ ì¶•ì œ(ì‹œì‘ì¼ > ì˜¤ëŠ˜) ì œì™¸\n",
    "- ì…ë ¥ ì¶•ì œ(ë˜ëŠ” ë§¤ì¹­ëª…) ì œì™¸\n",
    "- ë¶„ë¥˜ ìš°ì„ ìˆœìœ„: dataset â–¶ rule â–¶ llm(ì„ íƒ)\n",
    "- ì¶œë ¥:\n",
    "    1) íƒì§€ëœ [ëŒ€/ì¤‘/ì„¸ë¶€] ìœ í˜• (+ê·¼ê±°)\n",
    "    2) ë™ì¼ (ì„¸ë¶€â†’ì¤‘â†’ëŒ€) ìœ í˜• ì¶•ì œ ìˆ˜\n",
    "    3) ë™ì¼ ìœ í˜• ì¶•ì œ ëª©ë¡(ì´ë¦„ë§Œ)\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from difflib import get_close_matches\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------------- Env / OpenAI (ì„ íƒ) ----------------\n",
    "load_dotenv()\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "_client_mode = None\n",
    "try:\n",
    "    from openai import OpenAI  # >=1.x\n",
    "    _client = OpenAI()\n",
    "    _client_mode = \"new\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import openai               # <=0.x\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        _client = openai\n",
    "        _client_mode = \"legacy\"\n",
    "    except Exception:\n",
    "        _client = None\n",
    "        _client_mode = None\n",
    "\n",
    "# ---------------- íŒŒì¼ ê²½ë¡œ ----------------\n",
    "CANDIDATE_FILES = [\n",
    "    \"./csv/2024.csv\",\n",
    "    \"./csv/2025.csv\",\n",
    "    \"/mnt/data/2024.csv\",\n",
    "    \"/mnt/data/2025.csv\",\n",
    "]\n",
    "\n",
    "# ---------------- ë¶„ë¥˜ ì²´ê³„ ----------------\n",
    "ALLOWED_MAJOR = [\"ë¬¸í™”ì˜ˆìˆ \", \"ìì—°ìƒíƒœ\", \"ì „í†µì—­ì‚¬\", \"ì§€ì—­íŠ¹ì‚°ë¬¼\"]\n",
    "\n",
    "# ì„¸ë¶€ ê·œì¹™(í‚¤ì›Œë“œ) : ë‚˜ë¹„/ë°˜ë”§ë¶ˆ â‡’ 'ê³¤ì¶©'ë¡œ í¡ìˆ˜\n",
    "FINE_RULES: Dict[str, Dict[str, List[str]]] = {\n",
    "    \"ìì—°ìƒíƒœ\": {\n",
    "        \"ê½ƒ\": [\"ê½ƒ\",\"ë²šê½ƒ\",\"ì¥ë¯¸\",\"íŠ¤ë¦½\",\"ì½”ìŠ¤ëª¨ìŠ¤\",\"ìœ ì±„\",\"ë‹¨í’\",\"ì–µìƒˆ\"],\n",
    "        \"ê³¤ì¶©\": [\"ê³¤ì¶©\",\"ë‚˜ë¹„\",\"ë°˜ë”§ë¶ˆ\",\"ì• ë²Œë ˆ\",\"ì ìë¦¬\",\"ì‚¬ìŠ´ë²Œë ˆ\"],\n",
    "        \"ì¡°ë¥˜Â·ì² ìƒˆ\": [\"ì² ìƒˆ\",\"ë‘ë£¨ë¯¸\",\"ê¸°ëŸ¬ê¸°\",\"ë¬¼ìƒˆ\",\"ê°ˆë§¤ê¸°\"],\n",
    "        \"ì²œë¬¸Â·ë³„\": [\"ì²œë¬¸\",\"ë³„\",\"ì€í•˜\",\"ìœ ì„±\",\"ì²œì²´\",\"ì•¼ê²½\"],\n",
    "        \"ìˆ²Â·ìƒíƒœì›\": [\"ìƒíƒœ\",\"ìŠµì§€\",\"ìˆ²\",\"ìˆ˜ëª©ì›\",\"ìì—°\",\"ìƒíƒœì›\"],\n",
    "        \"ì‚°Â·ê³„ê³¡Â·ë¬¼\": [\"ì‚°\",\"ê³„ê³¡\",\"í˜¸ìˆ˜\",\"ê°•\",\"ë°”ë‹¤\",\"í•´ë³€\",\"ì„¬\"],\n",
    "    },\n",
    "    \"ì§€ì—­íŠ¹ì‚°ë¬¼\": {\n",
    "        \"ê³¼ì¼\": [\"ì‚¬ê³¼\",\"í¬ë„\",\"ë”¸ê¸°\",\"ìˆ˜ë°•\",\"ë°¤\",\"ë³µìˆ­ì•„\",\"ìë‘\",\"ê·¤\",\"ë§¤ì‹¤\",\"ë¸”ë£¨ë² ë¦¬\"],\n",
    "        \"ê³¡ë¬¼Â·ì±„ì†Œ\": [\"ê°ì\",\"ê³ êµ¬ë§ˆ\",\"ê³ ì¶”\",\"ì˜¥ìˆ˜ìˆ˜\",\"ìŒ€\",\"ë³´ë¦¬\",\"ì½©\",\"ë°°ì¶”\",\"ë¬´\",\"ì–‘íŒŒ\"],\n",
    "        \"ìˆ˜ì‚°Â·í•´ì‚°ë¬¼\": [\"êµ´\",\"ì¥ì–´\",\"ìˆ˜ì‚°\",\"í•´ì‚°ë¬¼\",\"ì „ë³µ\",\"ë‚™ì§€\",\"ë¬¸ì–´\",\"ë©ê²Œ\",\"ìƒˆìš°\",\"ê²Œ\",\"ë¯¸ì—­\",\"ê¹€\"],\n",
    "        \"ì¶•ì‚°\": [\"í•œìš°\",\"í•œëˆ\",\"ìš°ìœ \",\"ì¹˜ì¦ˆ\",\"ì–‘ê³ ê¸°\"],\n",
    "        \"ì£¼ë¥˜\": [\"ì™€ì¸\",\"ë§¥ì£¼\",\"ë§‰ê±¸ë¦¬\",\"ì „í†µì£¼\",\"ì†Œì£¼\",\"ìˆ \"],\n",
    "        \"ë””ì €íŠ¸Â·ì¹´í˜\": [\"ì»¤í”¼\",\"ë¹µ\",\"ë² ì´ì»¤ë¦¬\",\"ë””ì €íŠ¸\",\"ì¼€ì´í¬\",\"ì´ˆì½”\",\"ì¿ í‚¤\"],\n",
    "        \"ë°œíš¨Â·ê¹€ì¹˜\": [\"ê¹€ì¹˜\",\"ì “ê°ˆ\",\"ì¥\",\"ëœì¥\",\"ê³ ì¶”ì¥\",\"ê°„ì¥\",\"ì¥ì•„ì°Œ\"],\n",
    "    },\n",
    "    \"ë¬¸í™”ì˜ˆìˆ \": {\n",
    "        \"ìŒì•…Â·ì½˜ì„œíŠ¸\": [\"dj\",\"edm\",\"í™í•©\",\"ë©\",\"kpop\",\"ì¼€ì´íŒ\",\"ë®¤ì§\",\"ìŒì•…\",\"ì½˜ì„œíŠ¸\",\"í˜ìŠ¤í‹°ë²Œ\",\"ì¬ì¦ˆ\",\"í´ë˜ì‹\",\"ë²„ìŠ¤í‚¹\",\"í•©ì°½\",\"ì—°ì£¼\",\"ì˜¤ì¼€ìŠ¤íŠ¸ë¼\"],\n",
    "        \"ë¬´ìš©Â·ëŒ„ìŠ¤\": [\"ëŒ„ìŠ¤\",\"ë¬´ìš©\",\"ë¹„ë³´ì´\",\"ëŒ„ì‹±\"],\n",
    "        \"ì—°ê·¹Â·ë®¤ì§€ì»¬Â·ì˜í™”\": [\"ì—°ê·¹\",\"ë®¤ì§€ì»¬\",\"ì˜í™”\",\"ì˜í™”ì œ\",\"ì‹œë„¤ë§ˆ\"],\n",
    "        \"ì „ì‹œÂ·ë¯¸ìˆ Â·ì‚¬ì§„\": [\"ì „ì‹œ\",\"ë¯¸ìˆ \",\"ì•„íŠ¸\",\"ì‚¬ì§„\",\"ë¹„ì—”ë‚ ë ˆ\",\"íŠ¸ë¦¬ì—”ë‚ ë ˆ\"],\n",
    "        \"êµ­ì•…Â·ì „í†µê³µì—°\": [\"êµ­ì•…\",\"íŒì†Œë¦¬\",\"ì‚¬ë¬¼ë†€ì´\",\"í’ë¬¼\",\"íƒˆì¶¤\"],\n",
    "        \"ì‹œì¦ŒÂ·ê²¨ìš¸\": [\"ì‚°íƒ€\",\"í¬ë¦¬ìŠ¤ë§ˆìŠ¤\",\"ì—°ë§\",\"ì†¡ë…„\"],\n",
    "    },\n",
    "    \"ì „í†µì—­ì‚¬\": {\n",
    "        \"ì „í†µê³µì˜ˆ\": [\"ë„ìê¸°\",\"ë„ì˜ˆ\",\"ì˜¹ê¸°\",\"í•œì§€\",\"ì„œì˜ˆ\",\"ëª©ê³µ\",\"ê¸ˆì†ê³µì˜ˆ\",\"ì—¼ìƒ‰\",\"ì˜»ì¹ \"],\n",
    "        \"ìœ ì Â·ê±´ì¶•\": [\"í•œì˜¥\",\"ê³ íƒ\",\"ì„œì›\",\"í–¥êµ\",\"ìì„±\",\"ì„±ê³½\",\"ê³ ë¶„\",\"ì™•ë¦‰\",\"ì‚¬ì°°\",\"ê¶\"],\n",
    "        \"ë¯¼ì†Â·í–¥í† \": [\"ì „í†µ\",\"ì „í†µì¶•ì œ\",\"ë¯¼ì†\",\"í–¥í† \",\"ì„¸ì‹œ\",\"ì˜ë¡€\",\"ë‹¨ì˜¤\",\"ì •ì›”ëŒ€ë³´ë¦„\",\"í’ì–´ì œ\",\"ë‹¹ì‚°ì œ\"],\n",
    "        \"ë¬´í˜•ë¬¸í™”ì¬\": [\"ë¬´í˜•ë¬¸í™”ì¬\",\"êµ­ê°€ë¬´í˜•ë¬¸í™”ì¬\",\"ì „ìŠ¹\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ì—­ë§¤í•‘(ì„¸ë¶€ â†’ ëŒ€)\n",
    "FINE_TO_MAJOR = {fine: major for major, bucket in FINE_RULES.items() for fine in bucket.keys()}\n",
    "\n",
    "# ---------------- ìœ í‹¸ ----------------\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().replace(\"\\xa0\", \" \")\n",
    "    return re.sub(r\"\\s+\", \"\", s)\n",
    "\n",
    "def parse_date_str(s: str) -> Optional[pd.Timestamp]:\n",
    "    return pd.to_datetime(str(s), errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "\n",
    "def today_floor_ts() -> pd.Timestamp:\n",
    "    return pd.Timestamp.today().normalize()\n",
    "\n",
    "# ë™ì˜ì–´ë¥¼ í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì •ê·œí™”\n",
    "COL_SYNONYMS = {\n",
    "    \"ì—°ë²ˆ\": [\"ì—°ë²ˆ\", \"ë²ˆí˜¸\", \"id\", \"ID\"],\n",
    "    \"ê´‘ì—­\": [\"ê´‘ì—­ìì¹˜ë‹¨ì²´ëª…\",\"ê´‘ì—­\",\"ì‹œë„\",\"ê´‘ì—­ì‹œë„\",\"ê´‘ì—­ëª…\"],\n",
    "    \"ê¸°ì´ˆ\": [\"ê¸°ì´ˆìì¹˜ë‹¨ì²´ëª…\",\"ê¸°ì´ˆ\",\"ì‹œêµ°êµ¬\",\"ìì¹˜êµ¬\",\"ê¸°ì´ˆëª…\"],\n",
    "    \"ì¶•ì œëª…\": [\"ì¶•ì œëª…\",\"í–‰ì‚¬ëª…\",\"ì´ë²¤íŠ¸ëª…\",\"ëª…ì¹­\",\"íƒ€ì´í‹€\"],\n",
    "    \"ì‹œì‘ì¼\": [\"ì‹œì‘ì¼\",\"start\",\"ì‹œì‘\",\"start_date\",\"ì‹œì‘ì¼ì\",\"ê°œë§‰ì¼\"],\n",
    "    \"ì¢…ë£Œì¼\": [\"ì¢…ë£Œì¼\",\"end\",\"ì¢…ë£Œ\",\"end_date\",\"ì¢…ë£Œì¼ì\",\"íë§‰ì¼\"],\n",
    "    \"ëŒ€ë¶„ë¥˜\": [\"ëŒ€ë¶„ë¥˜\",\"ë¶„ë¥˜ëŒ€\",\"ìœ í˜•ëŒ€\",\"ì¶•ì œìœ í˜•(ëŒ€)\",\"ì¶•ì œìœ í˜•_ëŒ€\",\"ë¶„ë¥˜_ëŒ€\",\"ëŒ€ë¶„ë¥˜ëª…\"],\n",
    "    \"ì¤‘ë¶„ë¥˜\": [\"ì¤‘ë¶„ë¥˜\",\"ë¶„ë¥˜ì¤‘\",\"ìœ í˜•ì¤‘\",\"ì¶•ì œìœ í˜•(ì¤‘)\",\"ì¶•ì œìœ í˜•_ì¤‘\",\"ë¶„ë¥˜_ì¤‘\",\"ì¤‘ë¶„ë¥˜ëª…\"],\n",
    "    \"ì„¸ë¶€ë¶„ë¥˜\": [\"ì„¸ë¶€ë¶„ë¥˜\",\"ì„¸ë¶€ìœ í˜•\",\"ì†Œë¶„ë¥˜\",\"ì„¸ë¶€ë¶„ë¥˜\",\"ë¶„ë¥˜ì†Œ\",\"ìœ í˜•ì„¸ë¶€\",\"ì¶•ì œìœ í˜•(ì„¸ë¶€)\",\"ì¶•ì œìœ í˜•_ì„¸ë¶€\",\"ì„¸ë¶€ë¶„ë¥˜ëª…\"],\n",
    "    # êµ¬ë²„ì „ ë‹¨ì¼ì—´\n",
    "    \"êµ¬_ì¶•ì œìœ í˜•\": [\"ì¶•ì œìœ í˜•\",\"ì¶•ì œ ìœ í˜•\"],\n",
    "}\n",
    "\n",
    "def _pick_first_exist(colnames: List[str], df_cols: List[str]) -> Optional[str]:\n",
    "    s = set(df_cols)\n",
    "    for c in colnames:\n",
    "        if c in s: return c\n",
    "    return None\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    mapping = {}\n",
    "    cols = [str(c).strip() for c in df.columns]\n",
    "    for std, synonyms in COL_SYNONYMS.items():\n",
    "        found = _pick_first_exist(synonyms, cols)\n",
    "        if found:\n",
    "            mapping[found] = std\n",
    "    out = df.rename(columns=mapping).copy()\n",
    "    # í‘œì¤€ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ì»¬ëŸ¼ ì¶”ê°€\n",
    "    for need in [\"ì—°ë²ˆ\",\"ê´‘ì—­\",\"ê¸°ì´ˆ\",\"ì¶•ì œëª…\",\"ì‹œì‘ì¼\",\"ì¢…ë£Œì¼\",\"ëŒ€ë¶„ë¥˜\",\"ì¤‘ë¶„ë¥˜\",\"ì„¸ë¶€ë¶„ë¥˜\",\"êµ¬_ì¶•ì œìœ í˜•\"]:\n",
    "        if need not in out.columns:\n",
    "            out[need] = \"\"\n",
    "    # êµ¬ë²„ì „ ë‹¨ì¼ì—´(ì¶•ì œìœ í˜•) â†’ ëŒ€ë¶„ë¥˜/ì„¸ë¶€ë¶„ë¥˜ ì¶”ì •\n",
    "    if out[\"êµ¬_ì¶•ì œìœ í˜•\"].astype(str).str.strip().ne(\"\").mean() > 0:\n",
    "        # ê°’ì´ ëŒ€ë¶€ë¶„ ALLOWED_MAJORë©´ ëŒ€ë¶„ë¥˜ë¡œ, ì•„ë‹ˆë©´ ì„¸ë¶€ë¶„ë¥˜ë¡œ ê°„ì£¼\n",
    "        vals = out[\"êµ¬_ì¶•ì œìœ í˜•\"].astype(str).str.strip()\n",
    "        ratio_major = vals.isin(ALLOWED_MAJOR).mean()\n",
    "        if ratio_major >= 0.6:\n",
    "            out.loc[out[\"ëŒ€ë¶„ë¥˜\"].eq(\"\"), \"ëŒ€ë¶„ë¥˜\"] = vals\n",
    "        else:\n",
    "            out.loc[out[\"ì„¸ë¶€ë¶„ë¥˜\"].eq(\"\"), \"ì„¸ë¶€ë¶„ë¥˜\"] = vals\n",
    "    # ì„¸ë¶€ë¶„ë¥˜ â†’ ëŒ€ë¶„ë¥˜ ìë™ ì±„ì›€(ë£° í…Œì´ë¸” ê¸°ì¤€)\n",
    "    miss_major = out[\"ëŒ€ë¶„ë¥˜\"].astype(str).str.strip().eq(\"\")\n",
    "    guess = out.loc[miss_major, \"ì„¸ë¶€ë¶„ë¥˜\"].map(lambda x: FINE_TO_MAJOR.get(str(x).strip(), \"\"))\n",
    "    out.loc[miss_major & guess.astype(bool), \"ëŒ€ë¶„ë¥˜\"] = guess\n",
    "    return out\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    needed = [\"ì—°ë²ˆ\",\"ê´‘ì—­\",\"ê¸°ì´ˆ\",\"ì¶•ì œëª…\",\"ì‹œì‘ì¼\",\"ì¢…ë£Œì¼\",\"ëŒ€ë¶„ë¥˜\",\"ì¤‘ë¶„ë¥˜\",\"ì„¸ë¶€ë¶„ë¥˜\"]\n",
    "    for c in needed:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {c}\")\n",
    "    return df\n",
    "\n",
    "def read_many_csv(paths: List[str]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                df = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(p, dtype=str, encoding=\"utf-8-sig\").fillna(\"\")\n",
    "            df[\"_ì¶œì²˜íŒŒì¼\"] = Path(p).name\n",
    "            df = standardize_columns(df)\n",
    "            df = ensure_columns(df)\n",
    "            # í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” 'ì£¼ë¯¼í™”í•©'ì€ ì œê±°\n",
    "            mask = ~(\n",
    "                df[\"ëŒ€ë¶„ë¥˜\"].astype(str).str.contains(\"ì£¼ë¯¼í™”í•©\", na=False) |\n",
    "                df[\"ì¤‘ë¶„ë¥˜\"].astype(str).str.contains(\"ì£¼ë¯¼í™”í•©\", na=False) |\n",
    "                df[\"ì„¸ë¶€ë¶„ë¥˜\"].astype(str).str.contains(\"ì£¼ë¯¼í™”í•©\", na=False)\n",
    "            )\n",
    "            df = df[mask]\n",
    "            frames.append(df)\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(\"ì…ë ¥ CSVë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out = out.drop_duplicates(\n",
    "        subset=[\"ê´‘ì—­\",\"ê¸°ì´ˆ\",\"ì¶•ì œëª…\",\"ëŒ€ë¶„ë¥˜\",\"ì¤‘ë¶„ë¥˜\",\"ì„¸ë¶€ë¶„ë¥˜\",\"ì‹œì‘ì¼\",\"ì¢…ë£Œì¼\"],\n",
    "        keep=\"first\"\n",
    "    ).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ---------------- ë¶„ë¥˜(ë°ì´í„°ì…‹/ê·œì¹™/LLM) ----------------\n",
    "def _extract_json(text: str) -> Optional[dict]:\n",
    "    if not text: return None\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
    "    if not m: return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def dataset_detect(df: pd.DataFrame, name: str) -> Tuple[str,str,str, dict]:\n",
    "    \"\"\"ë°ì´í„°ì…‹ì—ì„œ [ëŒ€,ì¤‘,ì„¸ë¶€]ë¥¼ ì°¾ì•„ì„œ ë°˜í™˜. ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´.\"\"\"\n",
    "    exact = df[df[\"ì¶•ì œëª…\"].astype(str).str.strip() == name.strip()]\n",
    "    if exact.empty:\n",
    "        # ë¶€ë¶„ í¬í•¨\n",
    "        nname = normalize_text(name)\n",
    "        cand = df[df[\"ì¶•ì œëª…\"].astype(str).apply(lambda x: normalize_text(x).find(nname) >= 0 or nname.find(normalize_text(x)) >= 0)]\n",
    "        if not cand.empty:\n",
    "            exact = cand\n",
    "        else:\n",
    "            # ìœ ì‚¬ë„ ë§¤ì¹­\n",
    "            hits = get_close_matches(name, df[\"ì¶•ì œëª…\"].astype(str).tolist(), n=1, cutoff=0.75)\n",
    "            if hits:\n",
    "                exact = df[df[\"ì¶•ì œëª…\"] == hits[0]]\n",
    "\n",
    "    if exact.empty:\n",
    "        return \"\",\"\",\"\", {}\n",
    "\n",
    "    row = exact.iloc[0]\n",
    "    major = str(row.get(\"ëŒ€ë¶„ë¥˜\",\"\")).strip()\n",
    "    mid   = str(row.get(\"ì¤‘ë¶„ë¥˜\",\"\")).strip()\n",
    "    fine  = str(row.get(\"ì„¸ë¶€ë¶„ë¥˜\",\"\")).strip()\n",
    "\n",
    "    ev = {\n",
    "        \"source\": \"dataset\",\n",
    "        \"file\": str(row.get(\"_ì¶œì²˜íŒŒì¼\",\"\")),\n",
    "        \"ì—°ë²ˆ\": str(row.get(\"ì—°ë²ˆ\",\"\")),\n",
    "        \"matched_name\": str(row.get(\"ì¶•ì œëª…\",\"\")),\n",
    "    }\n",
    "    return major, mid, fine, ev\n",
    "\n",
    "def collect_rule_hits(name: str) -> Dict[str, Dict[str, List[str]]]:\n",
    "    nm = (name or \"\").lower()\n",
    "    nm_plain = re.sub(r\"\\s+\", \"\", nm)\n",
    "    hits: Dict[str, Dict[str, List[str]]] = {}\n",
    "    for major, bucket in FINE_RULES.items():\n",
    "        for fine, kws in bucket.items():\n",
    "            for kw in kws:\n",
    "                k = kw.lower()\n",
    "                if (k in nm) or (k in nm_plain):\n",
    "                    hits.setdefault(major, {}).setdefault(fine, [])\n",
    "                    if kw not in hits[major][fine]:\n",
    "                        hits[major][fine].append(kw)\n",
    "    return hits\n",
    "\n",
    "def rule_detect(name: str) -> Tuple[str,str,dict]:\n",
    "    \"\"\"ê·œì¹™ìœ¼ë¡œ [ëŒ€,ì„¸ë¶€] ì¶”ì •\"\"\"\n",
    "    hits = collect_rule_hits(name)\n",
    "    if not hits: return \"\",\"\", {}\n",
    "    # ì ìˆ˜í™”: fine hit ìˆ˜ê°€ ê°€ì¥ í° ì¡°í•©\n",
    "    best = None\n",
    "    best_score = -1\n",
    "    for major, fine_map in hits.items():\n",
    "        for fine, kws in fine_map.items():\n",
    "            score = len(kws)\n",
    "            if score > best_score:\n",
    "                best = (major, fine, kws)\n",
    "                best_score = score\n",
    "    if best is None: return \"\",\"\", {}\n",
    "    major, fine, kws = best\n",
    "    ev = {\"source\":\"rule\",\"major\":major,\"fine\":fine,\"hits\":\", \".join(kws)}\n",
    "    return major, fine, ev\n",
    "\n",
    "def llm_detect(name: str, allowed_fine: List[str], allowed_major: List[str]) -> Tuple[str,str,dict]:\n",
    "    if _client_mode is None or _client is None:\n",
    "        return \"\",\"\", {}\n",
    "    allowed_fine = sorted(list({f for f in allowed_fine if f and f not in [\"-\", \"\"]}))[:60]  # ì•ˆì „ìƒ 60ê°œ ì œí•œ\n",
    "    majors_str = \"|\".join(allowed_major)\n",
    "    fines_str  = \"|\".join(allowed_fine) if allowed_fine else \"\"\n",
    "    prompt = f\"\"\"\n",
    "í•œêµ­ ì¶•ì œëª…: \"{name}\"\n",
    "\n",
    "ì•„ë˜ JSONë§Œ ì¶œë ¥:\n",
    "{{\n",
    "  \"major\": \"{majors_str}\",\n",
    "  \"fine\": \"{fines_str if fines_str else '<ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´>'}\",\n",
    "  \"hint_keywords\": [\"<ì´ë¦„ì—ì„œ í¬ì°©í•œ íŒíŠ¸ ë‹¨ì–´ ìµœëŒ€ 3ê°œ>\"]\n",
    "}}\n",
    "ê·œì¹™:\n",
    "- majorëŠ” ë°˜ë“œì‹œ ìœ„ 4ê°œ ì¤‘ í•˜ë‚˜(ë¬¸í™”ì˜ˆìˆ /ìì—°ìƒíƒœ/ì „í†µì—­ì‚¬/ì§€ì—­íŠ¹ì‚°ë¬¼)\n",
    "- fineì€ ì œê³µëœ ëª©ë¡ì—ì„œ ê³ ë¥´ë˜, ì í•©í•œê²Œ ì—†ìœ¼ë©´ \"\"(ë¹ˆ ë¬¸ìì—´)\n",
    "- ì„¤ëª…ë¬¸ ê¸ˆì§€. JSONë§Œ.\n",
    "\"\"\"\n",
    "    try:\n",
    "        if _client_mode == \"new\":\n",
    "            resp = _client.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"Return ONLY compact JSON.\"},\n",
    "                    {\"role\":\"user\",\"content\":prompt},\n",
    "                ],\n",
    "                temperature=0, max_tokens=80,\n",
    "            )\n",
    "            content = resp.choices[0].message.content.strip()\n",
    "        else:\n",
    "            resp = _client.ChatCompletion.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"Return ONLY compact JSON.\"},\n",
    "                    {\"role\":\"user\",\"content\":prompt},\n",
    "                ],\n",
    "                temperature=0, max_tokens=80,\n",
    "            )\n",
    "            content = resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        data = _extract_json(content) or {}\n",
    "        major = str(data.get(\"major\",\"\")).strip()\n",
    "        fine  = str(data.get(\"fine\",\"\")).strip()\n",
    "        hints = data.get(\"hint_keywords\", [])\n",
    "        if not isinstance(hints, list): hints = []\n",
    "        ev = {\"source\":\"llm\",\"major\":major,\"fine\":fine,\"hints\":\", \".join(map(str,hints))}\n",
    "        return major, fine, ev\n",
    "    except Exception:\n",
    "        return \"\",\"\", {}\n",
    "\n",
    "# ---------------- íŒŒì´í”„ë¼ì¸ ----------------\n",
    "def detect_labels(df: pd.DataFrame, name: str) -> Tuple[str,str,str,dict]:\n",
    "    \"\"\"[ëŒ€,ì¤‘,ì„¸ë¶€, Evidence]\"\"\"\n",
    "    major, mid, fine, ev = dataset_detect(df, name)\n",
    "    if major or mid or fine:\n",
    "        # ì„¸ë¶€ê°€ ë¹„ì—ˆëŠ”ë° ë°ì´í„°ì— ì¤‘ë¶„ë¥˜ë§Œ ìˆìœ¼ë©´ fine<-ì¤‘ìœ¼ë¡œ ìŠ¹ê²©\n",
    "        if fine == \"\" and mid != \"\":\n",
    "            fine = mid\n",
    "        return major, mid, fine, {\"source\":\"dataset\", **ev}\n",
    "\n",
    "    # ê·œì¹™\n",
    "    r_major, r_fine, r_ev = rule_detect(name)\n",
    "    if r_major or r_fine:\n",
    "        # midëŠ” ì•„ì§ ëª¨ë¥´ë©´ ë¹ˆì¹¸\n",
    "        return r_major, \"\", r_fine, r_ev\n",
    "\n",
    "    # LLM (ì„ íƒ)\n",
    "    allowed_fine = sorted(df[\"ì„¸ë¶€ë¶„ë¥˜\"].astype(str).str.strip().unique().tolist())\n",
    "    l_major, l_fine, l_ev = llm_detect(name, allowed_fine, ALLOWED_MAJOR)\n",
    "    if l_major or l_fine:\n",
    "        return l_major, \"\", l_fine, l_ev\n",
    "\n",
    "    return \"\",\"\",\"\", {\"source\":\"none\"}\n",
    "\n",
    "def filter_same_group(df: pd.DataFrame, name: str, major: str, mid: str, fine: str) -> pd.DataFrame:\n",
    "    \"\"\"ë™ì¼ 'ì„¸ë¶€â†’ì¤‘â†’ëŒ€' ê·¸ë£¹ì—ì„œ, ì˜¤ëŠ˜ ì´ì „ ì‹œì‘ ì¶•ì œë§Œ, ìê¸° ìì‹  ì œì™¸\"\"\"\n",
    "    base = df.copy()\n",
    "    # ìš°ì„ ìˆœìœ„: ì„¸ë¶€ > ì¤‘ > ëŒ€\n",
    "    if fine:\n",
    "        mask = base[\"ì„¸ë¶€ë¶„ë¥˜\"].astype(str).str.strip() == fine\n",
    "    elif mid:\n",
    "        mask = base[\"ì¤‘ë¶„ë¥˜\"].astype(str).str.strip() == mid\n",
    "    else:\n",
    "        mask = base[\"ëŒ€ë¶„ë¥˜\"].astype(str).str.strip() == major\n",
    "    same = base[mask].copy()\n",
    "\n",
    "    # ì•„ì§ ì‹œì‘ ì•ˆ í•œ ì¶•ì œ ì œì™¸\n",
    "    today = today_floor_ts()\n",
    "    same[\"_ì‹œì‘\"] = pd.to_datetime(same[\"ì‹œì‘ì¼\"], errors=\"coerce\")\n",
    "    same = same[(~same[\"_ì‹œì‘\"].isna()) & (same[\"_ì‹œì‘\"] <= today)]\n",
    "\n",
    "    # ìê¸° ìì‹  ì œì™¸(ì´ë¦„ ì •ê·œí™”)\n",
    "    inorm = normalize_text(name)\n",
    "    same = same[ same[\"ì¶•ì œëª…\"].astype(str).apply(lambda x: normalize_text(x) != inorm) ]\n",
    "\n",
    "    same = same.sort_values(by=[\"ê´‘ì—­\",\"ê¸°ì´ˆ\",\"ì¶•ì œëª…\"]).drop(columns=[\"_ì‹œì‘\"], errors=\"ignore\")\n",
    "    return same[[\"ì—°ë²ˆ\",\"ê´‘ì—­\",\"ê¸°ì´ˆ\",\"ì¶•ì œëª…\",\"ëŒ€ë¶„ë¥˜\",\"ì¤‘ë¶„ë¥˜\",\"ì„¸ë¶€ë¶„ë¥˜\",\"ì‹œì‘ì¼\",\"ì¢…ë£Œì¼\",\"_ì¶œì²˜íŒŒì¼\"]]\n",
    "\n",
    "def run(festival_name: str, print_limit: int = 300, dedupe: bool = True):\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    df = read_many_csv(CANDIDATE_FILES)\n",
    "\n",
    "    # ë¼ë²¨ íƒì§€\n",
    "    major, mid, fine, ev = detect_labels(df, festival_name)\n",
    "\n",
    "    # ì¶œë ¥ í—¤ë”\n",
    "    print(f\"ì…ë ¥ ì¶•ì œëª…: {festival_name}\")\n",
    "    if fine or mid or major:\n",
    "        print(f\"íƒì§€ëœ ìœ í˜•: ëŒ€='{major or '-'}' | ì¤‘='{mid or '-'}' | ì„¸ë¶€='{fine or '-'}'  | ì¶œì²˜: {ev.get('source','-')}\")\n",
    "    else:\n",
    "        print(\"íƒì§€ëœ ìœ í˜•: (íƒì§€ ì‹¤íŒ¨)\")\n",
    "    # ê·¼ê±°\n",
    "    src = ev.get(\"source\",\"-\")\n",
    "    if src == \"dataset\":\n",
    "        print(f\"ê·¼ê±°(dataset): íŒŒì¼={ev.get('file','')}, ì—°ë²ˆ={ev.get('ì—°ë²ˆ','')}, ë§¤ì¹­ëª…={ev.get('matched_name','')}\")\n",
    "    elif src == \"rule\":\n",
    "        print(f\"ê·¼ê±°(rule): major='{ev.get('major','')}', fine='{ev.get('fine','')}', íˆíŠ¸=[{ev.get('hits','')}]\")\n",
    "    elif src == \"llm\":\n",
    "        print(f\"ê·¼ê±°(llm): major='{ev.get('major','')}', fine='{ev.get('fine','')}', íŒíŠ¸=[{ev.get('hints','')}]\")\n",
    "    else:\n",
    "        print(\"ê·¼ê±°: -\")\n",
    "\n",
    "    # ë™ì¼ ê·¸ë£¹ ì¶”ì¶œ\n",
    "    same = filter_same_group(df, festival_name, major, mid, fine)\n",
    "\n",
    "    if same.empty:\n",
    "        print(\"\\në™ì¼ ìœ í˜• ì¶•ì œ ìˆ˜: 0\")\n",
    "        return df, same\n",
    "\n",
    "    if dedupe:\n",
    "        same = same.drop_duplicates(subset=[\"ì¶•ì œëª…\"], keep=\"first\")\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ ì¶œë ¥(ì´ë¦„ë§Œ)\n",
    "    names = same.sort_values(by=[\"ì‹œì‘ì¼\",\"ì¶•ì œëª…\"])\n",
    "    names_list = names[\"ì¶•ì œëª…\"].astype(str).tolist()\n",
    "\n",
    "    print(f\"\\në™ì¼ ìœ í˜• ì¶•ì œ ìˆ˜: {len(names_list)}\")\n",
    "    print(\"\\n[ë™ì¼ ìœ í˜• ì¶•ì œ ëª©ë¡]\")\n",
    "    for i, nm in enumerate(names_list[:print_limit], start=1):\n",
    "        print(f\"{i:>3}. {nm}\")\n",
    "    if len(names_list) > print_limit:\n",
    "        print(f\"... ({len(names_list) - print_limit}ê°œ ë” ìˆìŒ)\")\n",
    "\n",
    "    return df, same\n",
    "\n",
    "# ------------- Example -------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ì˜ˆì‹œ: ì´ë¦„ë§Œ ë°”ê¿” í…ŒìŠ¤íŠ¸\n",
    "    _df, _same = run(\"ë‹´ì–‘ì‚°íƒ€ì¶•ì œ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_festival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
