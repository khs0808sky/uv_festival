{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d79529fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ ì¶•ì œëª…: ì„ì‹¤ ì‚°íƒ€ì¶•ì œ\n",
      "íƒì§€ëœ ì¶•ì œìœ í˜•: ì£¼ë¯¼í™”í•©  | ì¶œì²˜: dataset\n",
      "ê·¼ê±°(dataset): CSVì˜ 'ì¶•ì œ ìœ í˜•'='ì£¼ë¯¼í™”í•©' (íŒŒì¼: 2025_ì¶•ì œ_í•µì‹¬í•„ë“œ.csv, ì—°ë²ˆ: 965)\n",
      "íƒ€ê¹ƒ ì›”: 12,1,2 (ì‹œì¦Œ ì¶”ì •: -)\n",
      "\n",
      "ë™ì¼ ìœ í˜• & ê°™ì€ ì‹œê¸° ì¶•ì œ ìˆ˜: 8\n",
      "\n",
      "[ë™ì¼ ìœ í˜• Â· ìœ ì‚¬ ì‹œê¸°]\n",
      "  1. ì œ13íšŒ ê¸ˆì‚°ì²œ ë´„ê½ƒì¶•ì œ  2024-01-04 ~ 2024-01-04\n",
      "  2. ì œ13íšŒ ë‚¨ì¼ë©´ í™ë„í™”ì¶•ì œ  2024-01-04 ~ 2024-01-04\n",
      "  3. (ê°€ì œ) ìƒí”Œ 1Â·8ë¶€ë‘ ì•¼ì‹œì¥  2024-01-06 ~ 2024-01-06\n",
      "  4. ì œ11íšŒ ì•¼ë§¥ì¶•ì œ  2024-01-06 ~ 2024-01-06\n",
      "  5. ê¸°ì¥ê°¯ë§ˆì„ì¶•ì œ  2024-01-07 ~ 2024-01-08\n",
      "  6. ë™ì¸ì²œ ë‚­ë§Œì¶•ì œ  2024-01-10 ~ 2024-01-10\n",
      "  7. íƒœë°±ì œ   2024-01-10 ~ 2024-01-10\n",
      "  8. (ê°€ì¹­)ì˜¤ì‚° ì‚°íƒ€ ë§ˆì¼“(ì œ2íšŒ ì˜¤ì‚° í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ë§ˆì¼“)  2024-11-22 ~ 2024-12-24\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ğŸ¯ (2024+2025) ì¶•ì œëª… + ì›” â†’ (ë°ì´í„°/ê·œì¹™/LLM) ì¶•ì œìœ í˜• íŒì • + ê¸°ê°„(ì›”) í•„í„° â†’ 'ì¶•ì œëª…  ì‹œì‘ì¼ ~ ì¢…ë£Œì¼' ëª©ë¡\n",
    "- ë¡œë“œ: ./csv/{2024,2025}_ì¶•ì œ_í•µì‹¬í•„ë“œ.csv (+ /mnt/data ê²½ë¡œë„ ì‹œë„)\n",
    "- ì•„ì§ ì‹œì‘ ì•ˆ í•œ ì¶•ì œ(ì‹œì‘ì¼ > ì˜¤ëŠ˜) ê¸°ë³¸ ì œì™¸(ì˜µì…˜ìœ¼ë¡œ í•´ì œ ê°€ëŠ¥)\n",
    "- ì…ë ¥ ì¶•ì œ(ë˜ëŠ” ë§¤ì¹­ëª…) ê²°ê³¼ì—ì„œ ì œì™¸\n",
    "- ì›” ì…ë ¥ í¬ë§·: 3  |  \"3,4\"  |  \"3-5\"  |  \"ë´„/ì—¬ë¦„/ê°€ì„/ê²¨ìš¸\"\n",
    "- ì¶œë ¥:\n",
    "    1) íƒì§€ëœ ì¶•ì œìœ í˜•(+ì¶œì²˜) / íƒ€ê¹ƒ ì›”\n",
    "    2) ë™ì¼ ìœ í˜• & ê°™ì€ ì‹œê¸° ì¶•ì œ ìˆ˜\n",
    "    3) ì¶•ì œëª…  ì‹œì‘ì¼ ~ ì¢…ë£Œì¼\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict, Set, Union\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "try:\n",
    "    from pydantic import ConfigDict  # pydantic v2\n",
    "except ImportError:\n",
    "    ConfigDict = None\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ---------------- Env / OpenAI ----------------\n",
    "load_dotenv()\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "_client_mode = None\n",
    "try:\n",
    "    from openai import OpenAI  # >=1.x\n",
    "    _client = OpenAI()\n",
    "    _client_mode = \"new\"\n",
    "except Exception:\n",
    "    try:\n",
    "        import openai               # <=0.x\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        _client = openai\n",
    "        _client_mode = \"legacy\"\n",
    "    except Exception:\n",
    "        _client = None\n",
    "        _client_mode = None\n",
    "\n",
    "# ---------------- Settings --------------------\n",
    "ALLOWED_TYPES = [\"ë¬¸í™”ì˜ˆìˆ \", \"ì§€ì—­íŠ¹ì‚°ë¬¼\", \"ì£¼ë¯¼í™”í•©\", \"ìì—°ìƒíƒœ\", \"ì „í†µì—­ì‚¬\"]\n",
    "\n",
    "CANDIDATE_FILES = [\n",
    "    \"./csv/2024_ì¶•ì œ_í•µì‹¬í•„ë“œ.csv\",\n",
    "    \"./csv/2025_ì¶•ì œ_í•µì‹¬í•„ë“œ.csv\",\n",
    "    \"/mnt/data/2024_ì¶•ì œ_í•µì‹¬í•„ë“œ.csv\",\n",
    "    \"/mnt/data/2025_ì¶•ì œ_í•µì‹¬í•„ë“œ.csv\",\n",
    "]\n",
    "\n",
    "RULE_KEYWORDS: Dict[str, List[str]] = {\n",
    "    \"ë¬¸í™”ì˜ˆìˆ \": [\n",
    "        \"dj\",\"edm\",\"í™í•©\",\"ë©\",\"kpop\",\"ì¼€ì´íŒ\",\"ë®¤ì§\",\"ìŒì•…\",\"ì½˜ì„œíŠ¸\",\"í˜ìŠ¤í‹°ë²Œ\",\n",
    "        \"ëŒ„ìŠ¤\",\"ë¬´ìš©\",\"ë²„ìŠ¤í‚¹\",\"í•©ì°½\",\"ì—°ì£¼\",\"ì˜¤ì¼€ìŠ¤íŠ¸ë¼\",\"ì¬ì¦ˆ\",\"í´ë˜ì‹\",\n",
    "        \"ì—°ê·¹\",\"ë®¤ì§€ì»¬\",\"ì˜í™”\",\"ì „ì‹œ\",\"ë¯¸ìˆ \",\"ì•„íŠ¸\",\"ì‚¬ì§„\",\"ì„œì»¤ìŠ¤\",\"í¼í¬ë¨¼ìŠ¤\",\"ê³µì—°\",\n",
    "        \"êµ­ì•…\",\"ì‚°íƒ€\",\"í¬ë¦¬ìŠ¤ë§ˆìŠ¤\"\n",
    "    ],\n",
    "    \"ì§€ì—­íŠ¹ì‚°ë¬¼\": [\n",
    "        \"íŠ¹ì‚°\",\"ë¡œì»¬\",\"ë¨¹ê±°ë¦¬\",\"ë¯¸ì‹\",\"ë§›\",\"ì¥í„°\",\"ì‹œì¥\",\n",
    "        \"ì‚¬ê³¼\",\"í¬ë„\",\"ë”¸ê¸°\",\"ê°ì\",\"ê³ êµ¬ë§ˆ\",\"ìˆ˜ë°•\",\"ë°¤\",\"ê³ ì¶”\",\"ê¹€ì¹˜\",\n",
    "        \"êµ´\",\"ì¥ì–´\",\"ìˆ˜ì‚°\",\"í•´ì‚°ë¬¼\",\"í•œìš°\",\"í•œëˆ\",\"ìš°ìœ \",\"ì™€ì¸\",\"ë§¥ì£¼\",\"ë§‰ê±¸ë¦¬\",\"ì»¤í”¼\",\"ë¹µ\"\n",
    "    ],\n",
    "    \"ì£¼ë¯¼í™”í•©\": [\n",
    "        \"êµ°ë¯¼\",\"ì‹œë¯¼\",\"ë„ë¯¼\",\"ë©´ë¯¼\",\"ì£¼ë¯¼\",\"ìì¹˜\",\"í™”í•©\",\"ì–´ìš¸ë¦¼\",\"í•œë§ˆìŒ\",\n",
    "        \"í•œë§ˆë‹¹\",\"ì²´ìœ¡\",\"ë¯¼ì†ë†€ì´\",\"í•¨ê»˜\",\"ê°€ì¡±\",\"ë§ˆì„\",\"ê³µë™ì²´\",\"ì¶•ì „\"\n",
    "    ],\n",
    "    \"ìì—°ìƒíƒœ\": [\n",
    "        \"ê½ƒ\",\"ë²šê½ƒ\",\"ì¥ë¯¸\",\"íŠ¤ë¦½\",\"ì½”ìŠ¤ëª¨ìŠ¤\",\"ìœ ì±„\",\"ë‹¨í’\",\"ì–µìƒˆ\",\n",
    "        \"ë‚˜ë¹„\",\"ë°˜ë”§ë¶ˆ\",\"ê³¤ì¶©\",\"ì² ìƒˆ\",\"ìƒíƒœ\",\"ìŠµì§€\",\"ìˆ²\",\"ìˆ˜ëª©ì›\",\n",
    "        \"ì‚°\",\"ê³„ê³¡\",\"í˜¸ìˆ˜\",\"ë°”ë‹¤\",\"í•´ë³€\",\"ì„¬\",\"ì²œë¬¸\",\"ë³„\",\"ì•¼ê²½\",\"ìì—°\"\n",
    "    ],\n",
    "    \"ì „í†µì—­ì‚¬\": [\n",
    "        \"ì „í†µ\",\"ì „í†µì¶•ì œ\",\"ì—­ì‚¬\",\"ë¯¼ì†\",\"í–¥í† \",\"ë¬´í˜•ë¬¸í™”ì¬\",\"ë„ìê¸°\",\"ë„ì˜ˆ\",\"ì˜¹ê¸°\",\n",
    "        \"í•œì§€\",\"í•œë³µ\",\"ì„œì˜ˆ\",\"í•œì˜¥\",\"ê³ íƒ\",\"ì„œì›\",\"í–¥êµ\",\"ìì„±\",\"ì„±ê³½\",\"ê³ ë¶„\",\"ì™•ë¦‰\",\"íŒì†Œë¦¬\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ---------------- Utils (ê³µí†µ) -----------------------\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = str(s).strip().replace(\"\\xa0\", \" \")\n",
    "    return re.sub(r\"\\s+\", \"\", s)\n",
    "\n",
    "def parse_date_str(s: str) -> Optional[pd.Timestamp]:\n",
    "    try:\n",
    "        return pd.to_datetime(str(s), format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def today_floor_ts() -> pd.Timestamp:\n",
    "    return pd.Timestamp.today().normalize()\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    colmap = {}\n",
    "    for c in df.columns:\n",
    "        cc = str(c).strip()\n",
    "        if cc == \"ì¶•ì œìœ í˜•\":\n",
    "            colmap[c] = \"ì¶•ì œ ìœ í˜•\"\n",
    "    return df.rename(columns=colmap)\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame, need_dates: bool = True) -> pd.DataFrame:\n",
    "    df = standardize_columns(df)\n",
    "    needed = [\"ì—°ë²ˆ\", \"ê´‘ì—­ìì¹˜ë‹¨ì²´ëª…\", \"ê¸°ì´ˆìì¹˜ë‹¨ì²´ëª…\", \"ì¶•ì œëª…\", \"ì¶•ì œ ìœ í˜•\"]\n",
    "    for c in needed:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {c}\")\n",
    "    if need_dates and not {\"ì‹œì‘ì¼\", \"ì¢…ë£Œì¼\"}.issubset(df.columns):\n",
    "        raise ValueError(\"CSVì— 'ì‹œì‘ì¼','ì¢…ë£Œì¼' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "    return df\n",
    "\n",
    "def read_many_csv(paths: List[str]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if Path(p).exists():\n",
    "            try:\n",
    "                df = pd.read_csv(p, dtype=str).fillna(\"\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(p, dtype=str, encoding=\"utf-8-sig\").fillna(\"\")\n",
    "            df = ensure_columns(df, need_dates=True)\n",
    "            df[\"_ì¶œì²˜íŒŒì¼\"] = Path(p).name\n",
    "            frames.append(df)\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(\"ì…ë ¥ CSVë¥¼ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out = out.drop_duplicates(\n",
    "        subset=[\"ê´‘ì—­ìì¹˜ë‹¨ì²´ëª…\",\"ê¸°ì´ˆìì¹˜ë‹¨ì²´ëª…\",\"ì¶•ì œëª…\",\"ì¶•ì œ ìœ í˜•\",\"ì‹œì‘ì¼\",\"ì¢…ë£Œì¼\"],\n",
    "        keep=\"first\"\n",
    "    ).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ì›”/ê³„ì ˆ ë„ìš°ë¯¸\n",
    "def months_in_range(start: Optional[pd.Timestamp], end: Optional[pd.Timestamp]) -> Set[int]:\n",
    "    if pd.isna(start) or pd.isna(end): return set()\n",
    "    if end < start: start, end = end, start\n",
    "    months = set()\n",
    "    cur = pd.Timestamp(year=start.year, month=start.month, day=1)\n",
    "    last = pd.Timestamp(year=end.year, month=end.month, day=1)\n",
    "    while cur <= last:\n",
    "        months.add(int(cur.month))\n",
    "        if cur.month == 12:\n",
    "            cur = pd.Timestamp(year=cur.year + 1, month=1, day=1)\n",
    "        else:\n",
    "            cur = pd.Timestamp(year=cur.year, month=cur.month + 1, day=1)\n",
    "    return months\n",
    "\n",
    "def season_to_months(season: str) -> List[int]:\n",
    "    season = (season or \"\").strip()\n",
    "    return {\"ë´„\":[3,4,5], \"ì—¬ë¦„\":[6,7,8], \"ê°€ì„\":[9,10,11], \"ê²¨ìš¸\":[12,1,2]}.get(season, [])\n",
    "\n",
    "def parse_months_input(inp: Optional[Union[str,int,List[int]]]) -> List[int]:\n",
    "    \"\"\"3 | '3,4' | '3-5' | 'ë´„/ì—¬ë¦„/ê°€ì„/ê²¨ìš¸' â†’ [int,...]\"\"\"\n",
    "    if inp is None: return []\n",
    "    if isinstance(inp, list):\n",
    "        ms = []\n",
    "        for x in inp:\n",
    "            try:\n",
    "                xi = int(x)\n",
    "                if 1 <= xi <= 12: ms.append(xi)\n",
    "            except: pass\n",
    "        return sorted(list(dict.fromkeys(ms)))\n",
    "    if isinstance(inp, int):\n",
    "        return [inp] if 1 <= inp <= 12 else []\n",
    "    s = str(inp).strip()\n",
    "    if s in [\"ë´„\",\"ì—¬ë¦„\",\"ê°€ì„\",\"ê²¨ìš¸\"]:\n",
    "        return season_to_months(s)\n",
    "    s = s.replace(\" \", \"\")\n",
    "    if \"-\" in s:\n",
    "        a,b = s.split(\"-\",1)\n",
    "        try:\n",
    "            a=int(a); b=int(b)\n",
    "            if 1<=a<=12 and 1<=b<=12:\n",
    "                if a<=b: return list(range(a,b+1))\n",
    "                # 11-2 ê°™ì€ ë©ì–´ë¼ìš´ë“œ\n",
    "                seq = list(range(a,13))+list(range(1,b+1))\n",
    "                return seq\n",
    "        except: pass\n",
    "    # ì½¤ë§ˆ ë‚˜ì—´\n",
    "    ms=[]\n",
    "    for t in s.split(\",\"):\n",
    "        if not t: continue\n",
    "        try:\n",
    "            xi = int(t)\n",
    "            if 1<=xi<=12: ms.append(xi)\n",
    "        except: pass\n",
    "    return sorted(list(dict.fromkeys(ms)))\n",
    "\n",
    "# ---------------- ë¶„ë¥˜ ìœ í‹¸(ë°ì´í„°/ê·œì¹™/LLM) ----------------\n",
    "def detect_type_from_dataset(df: pd.DataFrame, name: str) -> Tuple[str, int, str, Optional[dict]]:\n",
    "    exact = df[df[\"ì¶•ì œëª…\"].astype(str).str.strip() == name.strip()]\n",
    "    if not exact.empty:\n",
    "        row = exact.iloc[0]; t = str(row[\"ì¶•ì œ ìœ í˜•\"]).strip()\n",
    "        evidence = {\"source\":\"dataset\",\"file\": str(row.get(\"_ì¶œì²˜íŒŒì¼\",\"\")), \"ì—°ë²ˆ\": str(row.get(\"ì—°ë²ˆ\",\"\"))}\n",
    "        return t, len(exact), str(row[\"ì¶•ì œëª…\"]), evidence\n",
    "\n",
    "    nname = normalize_text(name)\n",
    "    cand = df[df[\"ì¶•ì œëª…\"].astype(str).apply(\n",
    "        lambda x: normalize_text(x) in nname or nname in normalize_text(x)\n",
    "    )]\n",
    "    if not cand.empty:\n",
    "        row = cand.iloc[0]; t = str(row[\"ì¶•ì œ ìœ í˜•\"]).strip()\n",
    "        evidence = {\"source\":\"dataset\",\"file\": str(row.get(\"_ì¶œì²˜íŒŒì¼\",\"\")), \"ì—°ë²ˆ\": str(row.get(\"ì—°ë²ˆ\",\"\"))}\n",
    "        return t, len(cand), str(row[\"ì¶•ì œëª…\"]), evidence\n",
    "\n",
    "    all_names = df[\"ì¶•ì œëª…\"].astype(str).tolist()\n",
    "    hits = get_close_matches(name, all_names, n=1, cutoff=0.75)\n",
    "    if hits:\n",
    "        row = df[df[\"ì¶•ì œëª…\"] == hits[0]].iloc[0]\n",
    "        evidence = {\"source\":\"dataset\",\"file\": str(row.get(\"_ì¶œì²˜íŒŒì¼\",\"\")), \"ì—°ë²ˆ\": str(row.get(\"ì—°ë²ˆ\",\"\"))}\n",
    "        return str(row[\"ì¶•ì œ ìœ í˜•\"]).strip(), 1, str(row[\"ì¶•ì œëª…\"]), evidence\n",
    "    return \"\", 0, \"\", None\n",
    "\n",
    "def _extract_json(text: str) -> Optional[dict]:\n",
    "    if not text: return None\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
    "    if not m: return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def coerce_allowed(label: str) -> str:\n",
    "    label = (label or \"\").strip()\n",
    "    for t in ALLOWED_TYPES:\n",
    "        if t == label: return t\n",
    "    for t in ALLOWED_TYPES:\n",
    "        if t in label: return t\n",
    "    low = label.lower()\n",
    "    mapping = {\n",
    "        \"art\":\"ë¬¸í™”ì˜ˆìˆ \",\"music\":\"ë¬¸í™”ì˜ˆìˆ \",\"culture\":\"ë¬¸í™”ì˜ˆìˆ \",\n",
    "        \"food\":\"ì§€ì—­íŠ¹ì‚°ë¬¼\",\"local\":\"ì§€ì—­íŠ¹ì‚°ë¬¼\",\"specialty\":\"ì§€ì—­íŠ¹ì‚°ë¬¼\",\n",
    "        \"community\":\"ì£¼ë¯¼í™”í•©\",\"harmony\":\"ì£¼ë¯¼í™”í•©\",\n",
    "        \"nature\":\"ìì—°ìƒíƒœ\",\"eco\":\"ìì—°ìƒíƒœ\",\n",
    "        \"tradition\":\"ì „í†µì—­ì‚¬\",\"history\":\"ì „í†µì—­ì‚¬\",\"heritage\":\"ì „í†µì—­ì‚¬\",\n",
    "    }\n",
    "    for k,v in mapping.items():\n",
    "        if k in low: return v\n",
    "    return \"\"\n",
    "\n",
    "def llm_infer_type_with_evidence(name: str) -> Tuple[str, List[str]]:\n",
    "    if _client_mode is None or _client is None: return \"\", []\n",
    "    prompt = f\"\"\"\n",
    "í•œêµ­ì˜ ì¶•ì œëª…: \"{name}\"\n",
    "\n",
    "ì•„ë˜ JSON ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”(ì„¤ëª… ê¸ˆì§€):\n",
    "{{\n",
    "  \"label\": \"ë¬¸í™”ì˜ˆìˆ |ì§€ì—­íŠ¹ì‚°ë¬¼|ì£¼ë¯¼í™”í•©|ìì—°ìƒíƒœ|ì „í†µì—­ì‚¬\",\n",
    "  \"hint_keywords\": [\"<ì´ë¦„ì—ì„œ í¬ì°©í•œ íŒíŠ¸ ë‹¨ì–´ ìµœëŒ€ 3ê°œ>\"]\n",
    "}}\n",
    "- ë°˜ë“œì‹œ ìœ„ 5ê°œ ë¼ë²¨ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©\n",
    "- hint_keywordsëŠ” ì¶•ì œëª…ì— ì‹¤ì œë¡œ ë³´ì´ëŠ” ë‹¨ì–´/í‘œí˜„ ìœ„ì£¼ë¡œ ê°„ê²°íˆ\n",
    "\"\"\"\n",
    "    try:\n",
    "        if _client_mode == \"new\":\n",
    "            resp = _client.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"Output a compact JSON with the label and up to 3 hint keywords. No explanations.\"},\n",
    "                    {\"role\":\"user\",\"content\":prompt},\n",
    "                ],\n",
    "                temperature=0, max_tokens=60,\n",
    "            )\n",
    "            content = resp.choices[0].message.content.strip()\n",
    "        else:\n",
    "            resp = _client.ChatCompletion.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"Output a compact JSON with the label and up to 3 hint keywords. No explanations.\"},\n",
    "                    {\"role\":\"user\",\"content\":prompt},\n",
    "                ],\n",
    "                temperature=0, max_tokens=60,\n",
    "            )\n",
    "            content = resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        data = _extract_json(content) or {}\n",
    "        label = coerce_allowed(data.get(\"label\",\"\"))\n",
    "        hints  = data.get(\"hint_keywords\", [])\n",
    "        if not isinstance(hints, list): hints = []\n",
    "        hints = [str(h) for h in hints if isinstance(h, (str,int))]\n",
    "        return label, hints\n",
    "    except Exception:\n",
    "        return \"\", []\n",
    "\n",
    "def collect_name_keyword_hits(name: str) -> Dict[str, List[str]]:\n",
    "    nm = (name or \"\").lower()\n",
    "    nm_plain = re.sub(r\"\\s+\", \"\", nm)\n",
    "    hits: Dict[str, List[str]] = {k: [] for k in RULE_KEYWORDS.keys()}\n",
    "    for cat, kws in RULE_KEYWORDS.items():\n",
    "        seen=set()\n",
    "        for kw in kws:\n",
    "            k = kw.lower()\n",
    "            if (k in nm) or (k in nm_plain):\n",
    "                if k not in seen:\n",
    "                    hits[cat].append(kw); seen.add(k)\n",
    "    return {k:v for k,v in hits.items() if v}\n",
    "\n",
    "def rule_based_label(name_hits: Dict[str, List[str]]) -> str:\n",
    "    if not name_hits: return \"\"\n",
    "    strong = {\n",
    "        \"ì „í†µì—­ì‚¬\": {\"ì „í†µ\",\"ì „í†µì¶•ì œ\",\"ë¯¼ì†\",\"í–¥í† \",\"ë¬´í˜•ë¬¸í™”ì¬\",\"í•œì˜¥\",\"ì„œì›\",\"í–¥êµ\",\"ìì„±\",\"ì„±ê³½\",\"ì™•ë¦‰\",\"íŒì†Œë¦¬\"},\n",
    "        \"ìì—°ìƒíƒœ\": {\"ë²šê½ƒ\",\"ì¥ë¯¸\",\"íŠ¤ë¦½\",\"ì½”ìŠ¤ëª¨ìŠ¤\",\"ìœ ì±„\",\"ë‹¨í’\",\"ì–µìƒˆ\",\"ë‚˜ë¹„\",\"ë°˜ë”§ë¶ˆ\",\"ìƒíƒœ\",\"ìˆ²\",\"ìˆ˜ëª©ì›\"},\n",
    "        \"ì§€ì—­íŠ¹ì‚°ë¬¼\": {\"íŠ¹ì‚°\",\"ì¥í„°\",\"ì‹œì¥\",\"ë¯¸ì‹\",\"ë¨¹ê±°ë¦¬\",\"ì‚¬ê³¼\",\"í¬ë„\",\"ë”¸ê¸°\",\"ìˆ˜ì‚°\",\"ë§‰ê±¸ë¦¬\",\"ë§¥ì£¼\",\"ì»¤í”¼\"},\n",
    "        \"ë¬¸í™”ì˜ˆìˆ \": {\"ë®¤ì§\",\"ìŒì•…\",\"ì½˜ì„œíŠ¸\",\"ê³µì—°\",\"ì—°ê·¹\",\"ë®¤ì§€ì»¬\",\"ì „ì‹œ\",\"í˜ìŠ¤í‹°ë²Œ\",\"ì¬ì¦ˆ\",\"í´ë˜ì‹\",\"í™í•©\",\"kpop\",\"ì¼€ì´íŒ\"},\n",
    "        \"ì£¼ë¯¼í™”í•©\": {\"êµ°ë¯¼\",\"ì‹œë¯¼\",\"ë„ë¯¼\",\"ì£¼ë¯¼\",\"í™”í•©\",\"í•œë§ˆìŒ\",\"í•œë§ˆë‹¹\",\"ì¶•ì „\",\"ê³µë™ì²´\"},\n",
    "    }\n",
    "    scores = {k:0 for k in name_hits.keys()}\n",
    "    for cat,kws in name_hits.items():\n",
    "        for kw in kws:\n",
    "            k = kw.strip().lower()\n",
    "            scores[cat] += 3 if (kw in strong.get(cat,set()) or k in strong.get(cat,set())) else 1\n",
    "    ranked = sorted(scores.items(), key=lambda x:x[1], reverse=True)\n",
    "    top_cat, top_score = ranked[0]\n",
    "    second_score = ranked[1][1] if len(ranked)>1 else 0\n",
    "    if top_score==0: return \"\"\n",
    "    if top_score>=2 and top_score>=second_score+1: return top_cat\n",
    "    nonzero = [c for c,s in scores.items() if s>0]\n",
    "    if len(nonzero)==1: return nonzero[0]\n",
    "    return \"\"\n",
    "\n",
    "# --------- LLMìœ¼ë¡œ ì‹œê¸°(ê³„ì ˆ/ì›”) ì¶”ì • (ì›” ì…ë ¥ ì—†ì„ ë•Œ ë°±ì—…) ----------\n",
    "def llm_infer_period(name: str) -> Tuple[str, List[int]]:\n",
    "    if _client_mode is None or _client is None: return \"\", []\n",
    "    prompt = f\"\"\"\n",
    "í•œêµ­ì˜ ì¶•ì œëª…: \"{name}\"\n",
    "\n",
    "ë‹¤ìŒ í¬ë§·ì˜ JSONìœ¼ë¡œë§Œ ì¶œë ¥(ì„¤ëª… ê¸ˆì§€):\n",
    "{{\n",
    "  \"season\": \"ë´„|ì—¬ë¦„|ê°€ì„|ê²¨ìš¸|ë¶ˆëª…\",\n",
    "  \"months\": [<1..12 ì •ìˆ˜ë“¤ì˜ ë°°ì—´, ì¤‘ë³µX>]\n",
    "}}\n",
    "- ì¶•ì œëª…ì´ ì‹œê¸°ë¥¼ ê°•í•˜ê²Œ ì•”ì‹œí•˜ë©´ ê·¸ì— ë§ì¶° monthsë¥¼ 1~2ê°œì›”ë¡œ ì¢ê²Œ\n",
    "- ë¶ˆí™•ì‹¤í•˜ë©´ seasonë§Œ íŒë‹¨í•˜ê³  monthsëŠ” ë¹„ì›Œë„ ë¨\n",
    "\"\"\"\n",
    "    try:\n",
    "        if _client_mode == \"new\":\n",
    "            r = _client.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"Output only valid JSON.\"},\n",
    "                    {\"role\":\"user\",\"content\":prompt},\n",
    "                ],\n",
    "                temperature=0, max_tokens=60,\n",
    "            )\n",
    "            content = r.choices[0].message.content.strip()\n",
    "        else:\n",
    "            r = _client.ChatCompletion.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":\"Output only valid JSON.\"},\n",
    "                    {\"role\":\"user\",\"content\":prompt},\n",
    "                ],\n",
    "                temperature=0, max_tokens=60,\n",
    "            )\n",
    "            content = r[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        data = _extract_json(content) or {}\n",
    "        season = str(data.get(\"season\",\"\")).strip()\n",
    "        months = data.get(\"months\", [])\n",
    "        if not isinstance(months, list): months=[]\n",
    "        months = [int(m) for m in months if str(m).isdigit() and 1<=int(m)<=12]\n",
    "        if not months and season in [\"ë´„\",\"ì—¬ë¦„\",\"ê°€ì„\",\"ê²¨ìš¸\"]:\n",
    "            months = season_to_months(season)\n",
    "        # dedupe\n",
    "        seen=set(); ded=[]\n",
    "        for m in months:\n",
    "            if m not in seen: seen.add(m); ded.append(m)\n",
    "        return season, ded\n",
    "    except Exception:\n",
    "        return \"\", []\n",
    "\n",
    "# ---------------- State / Nodes ----------------\n",
    "class FestState(BaseModel):\n",
    "    if ConfigDict is not None:\n",
    "        model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    else:\n",
    "        class Config:\n",
    "            arbitrary_types_allowed = True\n",
    "\n",
    "    # ì…ë ¥\n",
    "    query_name: str = Field(\"\", description=\"ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¶•ì œëª…\")\n",
    "    target_months_input: Optional[Union[str,int,List[int]]] = None\n",
    "    csv_path_override: Optional[str] = None\n",
    "    exclude_upcoming: bool = True   # ì•„ì§ ì‹œì‘ ì•ˆ í•œ ì¶•ì œ ì œì™¸ ì—¬ë¶€\n",
    "\n",
    "    # ë‚´ë¶€\n",
    "    df: Optional[pd.DataFrame] = None\n",
    "    detected_type: str = \"\"\n",
    "    classification_source: str = \"\"    # dataset | rule | llm\n",
    "    matched_name: str = \"\"\n",
    "    evidence: Dict[str, str] = {}      # ë¬¸ìì—´ë§Œ\n",
    "    name_keyword_hits: Dict[str, List[str]] = {}\n",
    "    target_months: List[int] = []\n",
    "    detected_season: str = \"\"\n",
    "\n",
    "    # ì¶œë ¥\n",
    "    same_type_df: Optional[pd.DataFrame] = None\n",
    "    period_filtered_df: Optional[pd.DataFrame] = None\n",
    "\n",
    "def node_load_data(state: FestState) -> FestState:\n",
    "    if state.csv_path_override and Path(state.csv_path_override).exists():\n",
    "        df = pd.read_csv(state.csv_path_override, dtype=str).fillna(\"\")\n",
    "        df = ensure_columns(df, need_dates=True)\n",
    "        df[\"_ì¶œì²˜íŒŒì¼\"] = Path(state.csv_path_override).name\n",
    "    else:\n",
    "        df = read_many_csv(CANDIDATE_FILES)\n",
    "    state.df = df\n",
    "    return state\n",
    "\n",
    "def node_detect_type(state: FestState) -> FestState:\n",
    "    assert state.df is not None\n",
    "    state.name_keyword_hits = collect_name_keyword_hits(state.query_name)\n",
    "\n",
    "    # 1) ë°ì´í„°ì…‹\n",
    "    dtype, cnt, mname, dsevi = detect_type_from_dataset(state.df, state.query_name)\n",
    "    if dtype:\n",
    "        state.detected_type = dtype\n",
    "        state.classification_source = \"dataset\"\n",
    "        state.matched_name = mname\n",
    "        ev = {\"dataset_label\": dtype}\n",
    "        if dsevi: ev.update({k:str(v) for k,v in dsevi.items()})\n",
    "        state.evidence = ev\n",
    "        return state\n",
    "\n",
    "    # 2) ê·œì¹™\n",
    "    rule_label = rule_based_label(state.name_keyword_hits)\n",
    "    if rule_label:\n",
    "        state.detected_type = rule_label\n",
    "        state.classification_source = \"rule\"\n",
    "        state.matched_name = \"\"\n",
    "        hits_str = \", \".join(state.name_keyword_hits.get(rule_label, []))\n",
    "        state.evidence = {\"rule_label\": rule_label, \"rule_hits\": hits_str}\n",
    "        return state\n",
    "\n",
    "    # 3) LLM\n",
    "    label, hints = llm_infer_type_with_evidence(state.query_name)\n",
    "    state.detected_type = label\n",
    "    state.classification_source = \"llm\"\n",
    "    state.matched_name = \"\"\n",
    "    state.evidence = {\"llm_label\": label, \"llm_hint_keywords\": \", \".join(hints)}\n",
    "    return state\n",
    "\n",
    "def node_detect_period(state: FestState) -> FestState:\n",
    "    \"\"\"ì›” ì…ë ¥ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°ì´í„°/LLMë¡œ ì¶”ì •\"\"\"\n",
    "    assert state.df is not None\n",
    "    # 0) ì…ë ¥ ìš°ì„ \n",
    "    months = parse_months_input(state.target_months_input)\n",
    "    if months:\n",
    "        state.target_months = months\n",
    "        return state\n",
    "\n",
    "    # 1) ë°ì´í„°(ê°™ì€ ì´ë¦„ ë§¤ì¹­ í–‰)ì—ì„œ ê¸°ê°„ ì›” ì¶”ì¶œ\n",
    "    base_name = state.matched_name or state.query_name\n",
    "    row = state.df[state.df[\"ì¶•ì œëª…\"].astype(str).str.strip() == base_name.strip()]\n",
    "    if not row.empty:\n",
    "        sd = parse_date_str(row.iloc[0][\"ì‹œì‘ì¼\"])\n",
    "        ed = parse_date_str(row.iloc[0][\"ì¢…ë£Œì¼\"])\n",
    "        mm = months_in_range(sd, ed)\n",
    "        if mm:\n",
    "            state.target_months = sorted(list(mm))\n",
    "            # ì‹œì¦Œ íŒíŠ¸\n",
    "            if set(mm) & {3,4,5}: state.detected_season = state.detected_season or \"ë´„\"\n",
    "            if set(mm) & {6,7,8}: state.detected_season = state.detected_season or \"ì—¬ë¦„\"\n",
    "            if set(mm) & {9,10,11}: state.detected_season = state.detected_season or \"ê°€ì„\"\n",
    "            if set(mm) & {12,1,2}: state.detected_season = state.detected_season or \"ê²¨ìš¸\"\n",
    "            return state\n",
    "\n",
    "    # 2) LLM ì¶”ì •\n",
    "    season, mm_list = llm_infer_period(state.query_name)\n",
    "    state.detected_season = season or \"\"\n",
    "    state.target_months = mm_list or []\n",
    "    return state\n",
    "\n",
    "def node_filter_same_type(state: FestState) -> FestState:\n",
    "    assert state.df is not None\n",
    "    if not state.detected_type:\n",
    "        state.same_type_df = pd.DataFrame(); return state\n",
    "\n",
    "    same = state.df[state.df[\"ì¶•ì œ ìœ í˜•\"].astype(str).str.strip() == state.detected_type].copy()\n",
    "\n",
    "    # ì•„ì§ ì‹œì‘ ì•ˆ í•œ ì¶•ì œ ì œì™¸(ì˜µì…˜)\n",
    "    if state.exclude_upcoming:\n",
    "        today = today_floor_ts()\n",
    "        same[\"ì‹œì‘ì¼_dt\"] = same[\"ì‹œì‘ì¼\"].apply(parse_date_str)\n",
    "        same = same[(~same[\"ì‹œì‘ì¼_dt\"].isna()) & (same[\"ì‹œì‘ì¼_dt\"] <= today)]\n",
    "    else:\n",
    "        same[\"ì‹œì‘ì¼_dt\"] = same[\"ì‹œì‘ì¼\"].apply(parse_date_str)\n",
    "\n",
    "    # ìê¸° ìì‹  ì œì™¸\n",
    "    inorm = normalize_text(state.query_name)\n",
    "    mnorm = normalize_text(state.matched_name) if state.matched_name else \"\"\n",
    "    def _is_self_row(x: str) -> bool:\n",
    "        nx = normalize_text(x)\n",
    "        return (nx == inorm) or (bool(mnorm) and (nx == mnorm))\n",
    "    same = same[~same[\"ì¶•ì œëª…\"].astype(str).apply(_is_self_row)]\n",
    "\n",
    "    same = same.sort_values(by=[\"ê´‘ì—­ìì¹˜ë‹¨ì²´ëª…\",\"ê¸°ì´ˆìì¹˜ë‹¨ì²´ëª…\",\"ì¶•ì œëª…\"])\n",
    "    state.same_type_df = same[[\"ì—°ë²ˆ\",\"ê´‘ì—­ìì¹˜ë‹¨ì²´ëª…\",\"ê¸°ì´ˆìì¹˜ë‹¨ì²´ëª…\",\"ì¶•ì œëª…\",\"ì¶•ì œ ìœ í˜•\",\"ì‹œì‘ì¼\",\"ì¢…ë£Œì¼\",\"_ì¶œì²˜íŒŒì¼\",\"ì‹œì‘ì¼_dt\"]]\n",
    "    return state\n",
    "\n",
    "def node_filter_by_period(state: FestState) -> FestState:\n",
    "    df = state.same_type_df\n",
    "    if df is None or df.empty:\n",
    "        state.period_filtered_df = pd.DataFrame(); return state\n",
    "\n",
    "    months = set(int(m) for m in state.target_months if 1<=int(m)<=12)\n",
    "    if not months:\n",
    "        # íƒ€ê¹ƒ ì›” ì—†ìœ¼ë©´ ë™ì¼ ìœ í˜• ê·¸ëŒ€ë¡œ\n",
    "        state.period_filtered_df = df.drop(columns=[\"ì‹œì‘ì¼_dt\"], errors=\"ignore\")\n",
    "        return state\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"ì¢…ë£Œì¼_dt\"] = df[\"ì¢…ë£Œì¼\"].apply(parse_date_str)\n",
    "\n",
    "    def _hit(row) -> bool:\n",
    "        sd = row[\"ì‹œì‘ì¼_dt\"]; ed = row[\"ì¢…ë£Œì¼_dt\"]\n",
    "        rng = months_in_range(sd, ed)\n",
    "        return len(rng & months) > 0\n",
    "\n",
    "    df = df[(~df[\"ì‹œì‘ì¼_dt\"].isna()) & (~df[\"ì¢…ë£Œì¼_dt\"].isna()) & df.apply(_hit, axis=1)]\n",
    "    df = df.drop(columns=[\"ì‹œì‘ì¼_dt\",\"ì¢…ë£Œì¼_dt\"]).reset_index(drop=True)\n",
    "    state.period_filtered_df = df\n",
    "    return state\n",
    "\n",
    "def build_app():\n",
    "    g = StateGraph(FestState)\n",
    "    g.add_node(\"load_data\", node_load_data)\n",
    "    g.add_node(\"detect_type\", node_detect_type)\n",
    "    g.add_node(\"detect_period\", node_detect_period)\n",
    "    g.add_node(\"filter_same_type\", node_filter_same_type)\n",
    "    g.add_node(\"filter_by_period\", node_filter_by_period)\n",
    "\n",
    "    g.add_edge(START, \"load_data\")\n",
    "    g.add_edge(\"load_data\", \"detect_type\")\n",
    "    g.add_edge(\"detect_type\", \"detect_period\")\n",
    "    g.add_edge(\"detect_period\", \"filter_same_type\")\n",
    "    g.add_edge(\"filter_same_type\", \"filter_by_period\")\n",
    "    g.add_edge(\"filter_by_period\", END)\n",
    "    return g.compile()\n",
    "\n",
    "APP = build_app()\n",
    "\n",
    "def _to_state(obj) -> FestState:\n",
    "    if isinstance(obj, FestState): return obj\n",
    "    try:\n",
    "        return FestState.model_validate(obj)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return FestState.parse_obj(obj)\n",
    "        except Exception:\n",
    "            return FestState(**obj)\n",
    "\n",
    "# ---------------- Runner ----------------\n",
    "def run_pipeline_with_months(\n",
    "    festival_name: str,\n",
    "    months: Optional[Union[str,int,List[int]]] = None,\n",
    "    csv_path_override: Optional[str] = None,\n",
    "    print_limit: int = 300,\n",
    "    dedupe: bool = True,\n",
    "    exclude_upcoming: bool = True,\n",
    ") -> FestState:\n",
    "    init = FestState(\n",
    "        query_name=festival_name,\n",
    "        target_months_input=months,\n",
    "        csv_path_override=csv_path_override,\n",
    "        exclude_upcoming=exclude_upcoming,\n",
    "    )\n",
    "    raw = APP.invoke(init)\n",
    "    s = _to_state(raw)\n",
    "\n",
    "    # --- í—¤ë” ---\n",
    "    print(f\"ì…ë ¥ ì¶•ì œëª…: {festival_name}\")\n",
    "    print(f\"íƒì§€ëœ ì¶•ì œìœ í˜•: {s.detected_type or '(íƒì§€ ì‹¤íŒ¨)'}  | ì¶œì²˜: {s.classification_source or '-'}\")\n",
    "\n",
    "    # ê·¼ê±°\n",
    "    if s.classification_source == \"dataset\":\n",
    "        file = s.evidence.get(\"file\",\"\"); lab = s.evidence.get(\"dataset_label\",\"\"); no = s.evidence.get(\"ì—°ë²ˆ\",\"\")\n",
    "        print(f\"ê·¼ê±°(dataset): CSVì˜ 'ì¶•ì œ ìœ í˜•'='{lab}' (íŒŒì¼: {file}, ì—°ë²ˆ: {no})\")\n",
    "    elif s.classification_source == \"rule\":\n",
    "        lab = s.evidence.get(\"rule_label\",\"\"); hits = s.evidence.get(\"rule_hits\",\"\")\n",
    "        print(f\"ê·¼ê±°(rule): ê·œì¹™ ë¼ë²¨='{lab}', íˆíŠ¸ í‚¤ì›Œë“œ=[{hits}]\")\n",
    "    elif s.classification_source == \"llm\":\n",
    "        lab = s.evidence.get(\"llm_label\",\"\"); hints = s.evidence.get(\"llm_hint_keywords\",\"\")\n",
    "        print(f\"ê·¼ê±°(llm): ëª¨ë¸ ë¼ë²¨='{lab}', íŒíŠ¸ í‚¤ì›Œë“œ=[{hints}]\")\n",
    "\n",
    "    # ì›” ìš”ì•½\n",
    "    months_txt = \",\".join(str(m) for m in s.target_months) if s.target_months else \"-\"\n",
    "    season_txt = s.detected_season or \"-\"\n",
    "    print(f\"íƒ€ê¹ƒ ì›”: {months_txt} (ì‹œì¦Œ ì¶”ì •: {season_txt})\")\n",
    "\n",
    "    # ê²°ê³¼\n",
    "    df = s.period_filtered_df if s.period_filtered_df is not None else pd.DataFrame()\n",
    "    if not df.empty:\n",
    "        if dedupe:\n",
    "            df = df.drop_duplicates(subset=[\"ì¶•ì œëª…\"], keep=\"first\")\n",
    "        df[\"_sd\"] = pd.to_datetime(df[\"ì‹œì‘ì¼\"], errors=\"coerce\")\n",
    "        df = df.sort_values(by=[\"_sd\",\"ì¶•ì œëª…\"]).drop(columns=[\"_sd\"])\n",
    "        rows = [f\"{r['ì¶•ì œëª…']}  {r['ì‹œì‘ì¼']} ~ {r['ì¢…ë£Œì¼']}\" for _,r in df.iterrows()]\n",
    "    else:\n",
    "        rows = []\n",
    "\n",
    "    print(f\"\\në™ì¼ ìœ í˜• & ê°™ì€ ì‹œê¸° ì¶•ì œ ìˆ˜: {len(rows)}\")\n",
    "    if rows:\n",
    "        print(\"\\n[ë™ì¼ ìœ í˜• Â· ìœ ì‚¬ ì‹œê¸°]\")\n",
    "        for i, line in enumerate(rows[:print_limit], start=1):\n",
    "            print(f\"{i:>3}. {line}\")\n",
    "        if len(rows) > print_limit:\n",
    "            print(f\"... ({len(rows)-print_limit}ê°œ ë” ìˆìŒ)\")\n",
    "    return s\n",
    "\n",
    "# ------------- Example -------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ì˜ˆì‹œ 1) ì›” ëª…ì‹œ: 12ì›”\n",
    "    state = run_pipeline_with_months(\"ì„ì‹¤ ì‚°íƒ€ì¶•ì œ\", months=\"ê²¨ìš¸\")\n",
    "    # ì˜ˆì‹œ 2) ì›” ë²”ìœ„: \"11-12\"\n",
    "    # state = run_pipeline_with_months(\"ë‹´ì–‘ì‚°íƒ€ì¶•ì œ\", months=\"11-12\")\n",
    "    # ì˜ˆì‹œ 3) ê³„ì ˆ í‚¤ì›Œë“œ: \"ê²¨ìš¸\"\n",
    "    # state = run_pipeline_with_months(\"ë‹´ì–‘ì‚°íƒ€ì¶•ì œ\", months=\"ê²¨ìš¸\")\n",
    "    # ì˜ˆì‹œ 4) ì›” ë¯¸ì…ë ¥ â†’ ë°ì´í„°/LLMë¡œ ì¶”ì •\n",
    "    # state = run_pipeline_with_months(\"ë‹´ì–‘ì‚°íƒ€ì¶•ì œ\", months=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_festival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
